import streamlit as st
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input

# --- Configuraci칩n de la P치gina ---
st.set_page_config(
    page_title="Explorador de Modelos Predictivos",
    page_icon="游뱄",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Carga y Cacheo de Datos ---
@st.cache_data
def load_data():
    """
    Carga el dataset de California Housing, lo divide en conjuntos de entrenamiento y prueba,
    y escala las caracter칤sticas.
    """
    housing = fetch_california_housing()
    df = pd.DataFrame(housing.data, columns=housing.feature_names)
    df['MedHouseVal'] = housing.target
    
    # Scikit-learn a partir de la versi칩n 1.2 ya no incluye lat y lon en .data
    # Esta l칩gica asegura que se asignen correctamente.
    df['Latitude'] = housing.data[:, -2] if 'Latitude' not in housing.feature_names else df['Latitude']
    df['Longitude'] = housing.data[:, -1] if 'Longitude' not in housing.feature_names else df['Longitude']

    # Asegurar que las caracter칤sticas (X) no incluyan latitud, longitud ni el target.
    feature_names_to_use = [name for name in housing.feature_names if name not in ['Latitude', 'Longitude']]
    X = df[feature_names_to_use]
    y = df['MedHouseVal']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Escalar caracter칤sticas es una buena pr치ctica para muchos modelos
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Devolvemos tanto los datos escalados como los no escalados y el scaler
    return df, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler, feature_names_to_use

# --- Entrenamiento de Modelos (Cacheado) ---
@st.cache_resource
def train_model(model_name, X_train, y_train, params):
    """
    Entrena un modelo predictivo con los hiperpar치metros dados.
    Utiliza st.cache_resource para evitar re-entrenar en cada rerun.
    """
    if model_name == 'Regresi칩n Lineal':
        model = LinearRegression()
    elif model_name == '츼rbol de Decisi칩n':
        model = DecisionTreeRegressor(max_depth=params['max_depth'], min_samples_split=params['min_samples_split'], random_state=42)
    elif model_name == 'Random Forest':
        model = RandomForestRegressor(n_estimators=params['n_estimators'], max_depth=params['max_depth'], random_state=42)
    elif model_name == 'Gradient Boosting':
        model = GradientBoostingRegressor(n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], max_depth=params['max_depth'], random_state=42)
    elif model_name == 'Support Vector Regressor (SVR)':
        model = SVR(C=params['C'], kernel=params['kernel'], gamma=params['gamma'])

    elif model_name == 'Support Vector Regressor (SVR)':
        model = SVR(C=params['C'], kernel=params['kernel'], gamma=params['gamma'])
    
    # NUEVO BLOQUE PARA LA RED NEURONAL
    elif model_name == 'Red Neuronal (MLP)':
        # Definir la arquitectura del modelo
        model = Sequential([
            Input(shape=(X_train.shape[1],)), # Capa de entrada con el n칰mero de caracter칤sticas
            Dense(params['neurons_layer1'], activation='relu'), # 1ra Capa Oculta
            Dense(params['neurons_layer2'], activation='relu'), # 2da Capa Oculta
            Dense(1) # Capa de salida con 1 neurona para regresi칩n
        ])
        
        # Compilar el modelo
        optimizer = keras.optimizers.Adam(learning_rate=params['learning_rate'])
        model.compile(optimizer=optimizer, loss='mean_squared_error')
        
        # Entrenar el modelo (aqu칤 se realiza el entrenamiento real)
        # Nota: El return de la funci칩n ya hace el .fit(), pero en un caso real de Keras,
        # el .fit se llamar칤a aqu칤 y se devolver칤a el modelo entrenado.
        # Por ahora, solo definimos y compilamos el modelo. El .fit se har치 fuera.
        
        # Entrenar el modelo (El .fit para Keras es diferente)
        # Para integrarlo sin cambiar mucho la estructura, el .fit se llamar치 aqu칤
        # y devolveremos el modelo entrenado.
        model.fit(X_train, y_train, epochs=params['epochs'], batch_size=32, verbose=0)
        return model # Devuelve el modelo ya entrenado

    # El model.fit() que estaba fuera del if/elif ya no es necesario para Keras.
    # Vamos a reestructurar ligeramente para que funcione para todos.
    # if model_name != 'Red Neuronal (MLP)':
    #     model.fit(X_train, y_train)

# ... (c칩digo existente para otros modelos) ...
    elif model_name == 'Support Vector Regressor (SVR)':
        model = SVR(C=params['C'], kernel=params['kernel'], gamma=params['gamma'])
    
    # NUEVO BLOQUE PARA LA RED NEURONAL
    elif model_name == 'Red Neuronal (MLP)':
        # Definir la arquitectura del modelo
        model = Sequential([
            Input(shape=(X_train.shape[1],)), # Capa de entrada con el n칰mero de caracter칤sticas
            Dense(params['neurons_layer1'], activation='relu'), # 1ra Capa Oculta
            Dense(params['neurons_layer2'], activation='relu'), # 2da Capa Oculta
            Dense(1) # Capa de salida con 1 neurona para regresi칩n
        ])
        
        # Compilar el modelo
        optimizer = keras.optimizers.Adam(learning_rate=params['learning_rate'])
        model.compile(optimizer=optimizer, loss='mean_squared_error')
        
        # Entrenar el modelo (aqu칤 se realiza el entrenamiento real)
        # Nota: El return de la funci칩n ya hace el .fit(), pero en un caso real de Keras,
        # el .fit se llamar칤a aqu칤 y se devolver칤a el modelo entrenado.
        # Por ahora, solo definimos y compilamos el modelo. El .fit se har치 fuera.
        
        # Entrenar el modelo (El .fit para Keras es diferente)
        # Para integrarlo sin cambiar mucho la estructura, el .fit se llamar치 aqu칤
        # y devolveremos el modelo entrenado.
        model.fit(X_train, y_train, epochs=params['epochs'], batch_size=32, verbose=0)
        return model # Devuelve el modelo ya entrenado

    # El model.fit() que estaba fuera del if/elif ya no es necesario para Keras.
    # Vamos a reestructurar ligeramente para que funcione para todos.
    if model_name != 'Red Neuronal (MLP)':
        model.fit(X_train, y_train)

#    model.fit(X_train, y_train)
    return model


# --- Cargar todos los datos al inicio ---
df, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler, feature_names = load_data()


# --- Barra Lateral (Sidebar) ---
st.sidebar.title("Panel de Navegaci칩n")
app_mode = st.sidebar.selectbox(
    "Selecciona una secci칩n:",
    ["Introducci칩n", "An치lisis Exploratorio de Datos (EDA)", "Regresi칩n Lineal", 
     "츼rbol de Decisi칩n", "Random Forest", "Gradient Boosting", 
     "Support Vector Regressor (SVR)", "Red Neuronal (MLP)"] # <-- A칌ADIR AQU칈
)

st.sidebar.divider()

# CORRECCI칍N: Usar session_state para simular un modal y evitar el error de atributo.
# 1. Inicializar el estado si no existe.
if 'show_info_modal' not in st.session_state:
    st.session_state.show_info_modal = False

# 2. Bot칩n para ABRIR la gu칤a te칩rica.
if st.sidebar.button("Consultar info de Modelos", use_container_width=True):
    st.session_state.show_info_modal = True


# --- Contenedor de Informaci칩n Te칩rica (simula un modal) ---
# 3. Mostrar el contenido si el estado es True.
if st.session_state.show_info_modal:
    # Usamos un st.expander como un contenedor que se puede "cerrar".
    # Se mostrar치 en la parte superior del 치rea principal.
    with st.expander("Gu칤a Te칩rica de Modelos Predictivos e Hiperpar치metros", expanded=True):
        st.markdown("""
        ## Gu칤a Te칩rica de Modelos e Hiperpar치metros

        Esta gu칤a te ayudar치 a entender los conceptos clave detr치s de los modelos utilizados en esta aplicaci칩n.

        ---
        
        ### 쯈u칠 es un Hiperpar치metro?
        
        En Machine Learning, un **hiperpar치metro** es una configuraci칩n externa al modelo cuyo valor no se puede aprender de los datos. Es una perilla que el cient칤fico de datos ajusta *antes* de entrenar el modelo para controlar su comportamiento y rendimiento. La elecci칩n correcta de hiperpar치metros es crucial y a menudo requiere experimentaci칩n.

        ---

        ### 1. Regresi칩n Lineal

        - **Teor칤a:** Es el modelo m치s simple. Busca encontrar la mejor l칤nea recta (o hiperplano en m치s dimensiones) que se ajuste a los datos. Intenta modelar la relaci칩n entre las variables de entrada (X) y la variable de salida (y) mediante una ecuaci칩n lineal.
        - **F칩rmula:** $$ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon $$
        - **Hiperpar치metros en esta app:** Ninguno. La Regresi칩n Lineal est치ndar no tiene hiperpar치metros significativos que ajustar, lo que la hace un excelente punto de partida (baseline).

        ---

        ### 2. 츼rbol de Decisi칩n

        - **Teor칤a:** Funciona creando un modelo similar a un 치rbol de reglas de "si-entonces" que segmentan los datos. En cada nodo, el 치rbol elige la mejor caracter칤stica para dividir los datos y as칤 minimizar el error en las predicciones.
        - **Hiperpar치metros en esta app:**
            - `max_depth` (Profundidad M치xima): Controla cu치n profundo puede crecer el 치rbol. Un valor bajo puede causar subajuste (modelo demasiado simple), mientras que un valor muy alto puede causar sobreajuste (el modelo memoriza los datos).
            - `min_samples_split` (M칤nimo de Muestras para Dividir): Es el n칰mero m칤nimo de muestras que un nodo debe tener para poder ser dividido. Ayuda a prevenir el sobreajuste al no permitir divisiones en nodos con muy pocos datos.

        ---

        ### 3. Random Forest (Bosque Aleatorio)

        - **Teor칤a:** Es un m칠todo de *ensamble* que mejora la precisi칩n y controla el sobreajuste al entrenar m칰ltiples 츼rboles de Decisi칩n. Cada 치rbol se entrena con una muestra aleatoria de los datos y de las caracter칤sticas. La predicci칩n final es el promedio de las predicciones de todos los 치rboles individuales.
        - **Hiperpar치metros en esta app:**
            - `n_estimators` (N칰mero de 츼rboles): La cantidad de 치rboles que se construir치n en el bosque. Generalmente, m치s 치rboles mejoran el rendimiento, pero a un costo computacional mayor.
            - `max_depth`: Al igual que en un 치rbol de decisi칩n individual, controla la profundidad m치xima de cada 치rbol en el bosque.

        ---

        ### 4. Gradient Boosting

        - **Teor칤a:** Es otro m칠todo de *ensamble* que construye modelos (generalmente 치rboles) de forma secuencial. A diferencia de Random Forest, cada nuevo 치rbol se entrena para corregir los errores cometidos por los 치rboles anteriores. Es un m칠todo muy potente que a menudo resulta en modelos de alto rendimiento.
        - **Hiperpar치metros en esta app:**
            - `n_estimators`: El n칰mero de 치rboles (etapas de boosting) que se realizar치n.
            - `learning_rate` (Tasa de Aprendizaje): Reduce la contribuci칩n de cada 치rbol. Valores m치s bajos requieren m치s 치rboles para el mismo rendimiento, pero a menudo resultan en una mejor generalizaci칩n. Su valor est치 entre 0.0 y 1.0.
            - `max_depth`: La profundidad m치xima de cada 치rbol individual.

        ---

        ### 5. Support Vector Regressor (SVR)

        - **Teor칤a:** A diferencia de otros modelos que intentan minimizar el error, SVR intenta ajustar la mayor cantidad de datos posible *dentro* de un margen o "tubo" definido por un hiperpar치metro 칠psilon (no ajustable en esta app). Los puntos fuera de este tubo son penalizados. Es muy efectivo en espacios de caracter칤sticas de alta dimensi칩n.
        - **Hiperpar치metros en esta app:**
            - `C` (Par치metro de Regularizaci칩n): Controla la penalizaci칩n por los puntos que quedan fuera del margen. Un valor alto de `C` intenta ajustar m치s datos correctamente (menos errores), arriesg치ndose a un sobreajuste. Un valor bajo permite m치s errores (un margen m치s "suave").
            - `kernel`: Define la funci칩n utilizada para transformar los datos a una dimensi칩n superior y encontrar la mejor relaci칩n.
                - `linear`: Para relaciones lineales.
                - `poly`: Para relaciones polin칩micas.
                - `rbf` (Radial Basis Function): Muy popular y flexible para relaciones no lineales complejas. Su f칩rmula es: $$ K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2) $$
            - `gamma`: Un par치metro del kernel `rbf`. Define cu치nta influencia tiene un solo ejemplo de entrenamiento. Valores bajos significan una influencia lejana, mientras que valores altos significan una influencia cercana.

        ---

        ### 6. Red Neuronal (Perceptr칩n Multicapa - MLP)

        - **Teor칤a:** Inspirada en el cerebro humano, una red neuronal aprende a reconocer patrones complejos en los datos a trav칠s de capas de "neuronas" interconectadas. Es extremadamente potente y flexible, capaz de modelar relaciones no lineales muy complejas. Para regresi칩n, la red ajusta sus conexiones internas (pesos) para minimizar el error entre los precios predichos y los reales.
        - **Hiperpar치metros en esta app:**
            - `learning_rate` (Tasa de Aprendizaje): Controla cu치n grandes son los ajustes a los pesos del modelo durante el entrenamiento. Es un valor peque침o y crucial para una convergencia estable.
            - `epochs` (칄pocas): El n칰mero de veces que el modelo ver치 el conjunto de datos de entrenamiento completo. M치s 칠pocas permiten un mayor aprendizaje, pero tambi칠n aumentan el riesgo de sobreajuste.
            - `Neuronas en Capa Oculta 1 / 2`: Define el n칰mero de nodos de c칩mputo en cada una de las dos capas ocultas. M치s neuronas permiten al modelo aprender patrones m치s complejos, pero aumentan la complejidad y el riesgo de sobreajuste.

        """)
        # 4. Bot칩n para CERRAR la gu칤a.
        if st.button("Cerrar Gu칤a"):
            st.session_state.show_info_modal = False
            st.rerun() # Fuerza un rerun para que el expander desaparezca inmediatamente.

# --- L칩gica de P치ginas ---
if app_mode == "Introducci칩n":
    st.title("游뱄 Explorador Interactivo de Modelos Predictivos")
    st.markdown("""
    춰Bienvenido a esta aplicaci칩n interactiva para explorar, entrenar y evaluar diversos modelos predictivos de Machine Learning!
    
    Esta herramienta est치 dise침ada como una pr치ctica para entender los conceptos de modelado en un contexto de Big Data, utilizando el famoso dataset **"California Housing"**. El objetivo es predecir el **valor mediano de una vivienda** en California basado en diferentes caracter칤sticas.

    ### 쯈u칠 puedes hacer aqu칤?
    
    1.  **Navegar por las Secciones:** Utiliza el men칰 en el panel lateral para moverte entre las diferentes partes de la aplicaci칩n.
    2.  **An치lisis Exploratorio de Datos (EDA):** Entiende la estructura y distribuci칩n de los datos con tablas y visualizaciones.
    3.  **Explorar Modelos Predictivos:**
        - Selecciona un modelo de la lista (Regresi칩n Lineal, 츼rbol de Decisi칩n, etc.).
        - Lee una breve explicaci칩n sobre c칩mo funciona cada modelo.
        - **Ajusta los hiperpar치metros** del modelo usando los controles interactivos en el panel lateral.
        - Observa c칩mo cambian las m칠tricas de rendimiento (Error Cuadr치tico Medio y R) en tiempo real.
    4.  **Realizar Predicciones:** En la secci칩n de cada modelo, puedes introducir tus propios valores para las caracter칤sticas de una vivienda y obtener una predicci칩n instant치nea del precio.

    **춰Comienza explorando el dataset en la secci칩n de "An치lisis Exploratorio de Datos (EDA)" o salta directamente a entrenar un modelo!**
    """)

elif app_mode == "An치lisis Exploratorio de Datos (EDA)":
    st.title("游늵 An치lisis Exploratorio de Datos (EDA)")
    st.header("Dataset: California Housing")
    st.markdown("Este dataset contiene informaci칩n de los grupos de bloques censales en California. Cada fila corresponde a un grupo de bloques.")
    
    st.subheader("Vistazo a los Datos")
    st.dataframe(df.head())
    
    st.subheader("Descripci칩n de las Caracter칤sticas")
    st.markdown("""
    - **MedInc:** Ingreso mediano en el grupo de bloques (en decenas de miles de d칩lares).
    - **HouseAge:** Antig칲edad mediana de las viviendas en el grupo de bloques.
    - **AveRooms:** N칰mero promedio de habitaciones por vivienda.
    - **AveBedrms:** N칰mero promedio de dormitorios por vivienda.
    - **Population:** Poblaci칩n del grupo de bloques.
    - **AveOccup:** Ocupaci칩n promedio por vivienda (miembros por hogar).
    - **Latitude:** Latitud del centro del grupo de bloques.
    - **Longitude:** Longitud del centro del grupo de bloques.
    - **MedHouseVal (Target):** Valor mediano de la vivienda en el grupo de bloques (en cientos de miles de d칩lares).
    """)

    st.subheader("Descripci칩n Estad칤stica de los Datos")
    st.write(df.describe())

    st.subheader("Visualizaciones")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### Distribuci칩n del Valor Mediano de la Vivienda (Target)")
        st.markdown("El valor est치 en cientos de miles de d칩lares ($100,000s).")
        st.bar_chart(df['MedHouseVal'].value_counts().sort_index())
    
    with col2:
        st.markdown("#### Ubicaci칩n Geogr치fica de las Viviendas")
        st.markdown("El color representa el valor mediano de la vivienda (m치s oscuro = m치s caro).")
        
        map_df = df.copy()
        
        min_val = map_df['MedHouseVal'].min()
        max_val = map_df['MedHouseVal'].max()
        map_df['alpha'] = ((map_df['MedHouseVal'] - min_val) / (max_val - min_val)) * 255
        
        map_df['color'] = map_df['alpha'].apply(lambda alpha: [255, 90, 0, int(alpha)])
        
        map_df['size'] = map_df['Population'] * 0.01

        st.map(map_df, latitude='Latitude', longitude='Longitude', color='color', size='size')

else:
    model_name = app_mode
    st.title(f"游뱄 Modelo Predictivo: {model_name}")

    model_descriptions = {
        "Regresi칩n Lineal": "Un modelo simple que busca la mejor relaci칩n lineal entre las caracter칤sticas y el valor de la vivienda. Es r치pido y f치cil de interpretar, pero puede no capturar relaciones complejas.",
        "츼rbol de Decisi칩n": "Este modelo aprende una serie de reglas de 'si-entonces' para dividir los datos y hacer una predicci칩n. Es muy interpretable, pero propenso a sobreajustarse (memorizar) a los datos de entrenamiento.",
        "Random Forest": "Un modelo de ensamble que entrena muchos 츼rboles de Decisi칩n sobre diferentes subconjuntos de datos y promedia sus predicciones. Es mucho m치s robusto y preciso que un solo 치rbol, pero menos interpretable.",
        "Gradient Boosting": "Otro modelo de ensamble que construye 치rboles de forma secuencial. Cada nuevo 치rbol intenta corregir los errores de los 치rboles anteriores. Suele ser uno de los modelos de mayor rendimiento.",
        "Support Vector Regressor (SVR)": "Busca encontrar un 'tubo' que contenga la mayor cantidad de puntos de datos posible, minimizando las desviaciones fuera de 칠l. Es muy efectivo en espacios de alta dimensi칩n y cuando las relaciones no son lineales, especialmente con el kernel 'rbf'."
    }
    st.markdown(model_descriptions.get(model_name, "Selecciona un modelo para ver su descripci칩n."))
    st.divider()

    st.sidebar.header("Ajuste de Hiperpar치metros")
    params = {}
    if model_name == '츼rbol de Decisi칩n':
        params['max_depth'] = st.sidebar.slider("Profundidad M치xima (max_depth)", 1, 30, 10)
        params['min_samples_split'] = st.sidebar.slider("M칤nimo de Muestras para Dividir (min_samples_split)", 2, 100, 2)
    elif model_name == 'Random Forest':
        params['n_estimators'] = st.sidebar.slider("N칰mero de 츼rboles (n_estimators)", 10, 500, 100, step=10)
        params['max_depth'] = st.sidebar.slider("Profundidad M치xima (max_depth)", 1, 30, 10)
    elif model_name == 'Gradient Boosting':
        params['n_estimators'] = st.sidebar.slider("N칰mero de 츼rboles (n_estimators)", 10, 500, 100, step=10)
        params['learning_rate'] = st.sidebar.slider("Tasa de Aprendizaje (learning_rate)", 0.01, 1.0, 0.1, step=0.01)
        params['max_depth'] = st.sidebar.slider("Profundidad M치xima (max_depth)", 1, 10, 3)
    elif model_name == 'Support Vector Regressor (SVR)':
        params['C'] = st.sidebar.slider("Par치metro de Regularizaci칩n (C)", 0.01, 10.0, 1.0)
        params['kernel'] = st.sidebar.selectbox("Kernel", ['linear', 'rbf', 'poly'])
        params['gamma'] = st.sidebar.select_slider("Gamma", options=['scale', 'auto', 0.01, 0.1, 1])
    elif model_name == 'Red Neuronal (MLP)':
        params['learning_rate'] = st.sidebar.slider("Tasa de Aprendizaje (learning_rate)", 0.0001, 0.01, 0.001, format="%.4f")
        params['epochs'] = st.sidebar.slider("칄pocas de Entrenamiento (epochs)", 10, 200, 50)
        st.sidebar.subheader("Arquitectura de la Red")
        params['neurons_layer1'] = st.sidebar.slider("Neuronas en Capa Oculta 1", 16, 128, 64, step=16)
        params['neurons_layer2'] = st.sidebar.slider("Neuronas en Capa Oculta 2", 8, 64, 32, step=8)
        
    X_train_to_use = X_train_scaled if model_name == 'Support Vector Regressor (SVR)' else X_train
    X_test_to_use = X_test_scaled if model_name == 'Support Vector Regressor (SVR)' else X_test

    with st.spinner("Entrenando el modelo con los par치metros seleccionados..."):
        model = train_model(model_name, X_train_to_use, y_train, params)
    st.success("춰Modelo entrenado exitosamente!")

    y_pred = model.predict(X_test_to_use)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    st.header("Evaluaci칩n del Modelo")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Error Cuadr치tico Medio (MSE)", f"{mse:.4f}")
        st.info("游눠 **MSE:** Mide el promedio de los errores al cuadrado. Un valor m치s bajo es mejor.")
    with col2:
        st.metric("Coeficiente de Determinaci칩n (R)", f"{r2:.4f}")
        st.info("游눠 **R:** Indica qu칠 proporci칩n de la varianza en el valor de la vivienda es predecible a partir de las caracter칤sticas. Un valor m치s cercano a 1 es mejor.")

    st.divider()
    st.header("Realizar una Predicci칩n en Vivo")
    st.markdown("Ajusta los siguientes valores para simular una nueva vivienda y predecir su precio.")
    
    input_data = {}
    prediction_cols = st.columns(2)
    
    for i, feature in enumerate(feature_names):
        with prediction_cols[i % 2]:
            mean_val = df[feature].mean()
            std_val = df[feature].std()
            min_val = df[feature].min()
            max_val = df[feature].max()
            
            input_data[feature] = st.slider(
                f"Valor para {feature}",
                min_value=float(min_val),
                max_value=float(max_val),
                value=float(mean_val),
                step=float(std_val / 10)
            )

    if st.button("Predecir Valor de la Vivienda"):
        input_df = pd.DataFrame([input_data])
        
        if model_name == 'Support Vector Regressor (SVR)':
            input_df_scaled = scaler.transform(input_df)
            prediction = model.predict(input_df_scaled)
        else:
            prediction = model.predict(input_df)
        
        st.subheader("Resultado de la Predicci칩n")
        st.success(f"El valor mediano estimado para la vivienda es: **${prediction[0] * 100000:,.2f}**")

    with st.expander("Ver el c칩digo de Python para este modelo"):
        code = f"""
        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import StandardScaler
        from sklearn.{'linear_model' if model_name == 'Regresi칩n Lineal' else 'tree' if model_name == '츼rbol de Decisi칩n' else 'ensemble' if model_name in ['Random Forest', 'Gradient Boosting'] else 'svm'}.{model.__class__.__name__}
        from sklearn.metrics import mean_squared_error, r2_score

        # Nota: X_train, y_train, X_test, y_test son los datos pre-divididos.
        # Para SVR, se usar칤an X_train_scaled y X_test_scaled.

        # 1. Inicializar el modelo con los par치metros seleccionados
        params = {params}
        model = {model.__class__.__name__}(**params, random_state=42) if 'random_state' in {model.__class__}().get_params() else {model.__class__.__name__}(**params)

        # 2. Entrenar el modelo
        model.fit(X_train, y_train)

        # 3. Realizar predicciones en el conjunto de prueba
        y_pred = model.predict(X_test)

        # 4. Evaluar el rendimiento
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        print(f"MSE: {{mse:.4f}}")
        print(f"R: {{r2:.4f}}")
        """
        st.code(code, language='python')
